
# OBSERVABILITY HOST: Komodo + Monitoring Stack
# Build services one at a time in this order:
# 1. Komodo (container management - requires MongoDB from data host)
# 2. Prometheus (metrics storage)
# 3. Grafana (visualization)
# 4. Loki (log aggregation - requires MinIO from data host)
# 5. Alloy (metrics/logs collector)

services:
  # ============================================================================
  # 1. PORTAINER - Container Management UI
  # ============================================================================
  # Deploy FIRST - no dependencies
  # Access: https://10.10.10.112:9443

  portainer:
    image: portainer/portainer-ce:latest
    container_name: portainer
    restart: unless-stopped
    ports:
      - "10.10.10.112:9443:9443"  # HTTPS Web UI
      - "10.10.10.112:8000:8000"  # Edge agent tunnel
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./portainer/data:/data
    networks:
      - obs_net
    # No health check - minimal distroless container without shell/utilities
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================================================
  # 2. PROMETHEUS - Metrics Storage
  # ============================================================================
  # Deploy second - no dependencies
  # Access: http://10.10.10.112:9090 or https://prometheus.onurx.com

  prometheus:
    image: prom/prometheus:v3.1.0  # Latest stable version (2025)
    container_name: prometheus
    restart: unless-stopped
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=90d'  # 90 days retention
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-remote-write-receiver'  # Enable remote write (for Alloy)
      - '--web.enable-lifecycle'  # Enable reload via API
    ports:
      - "10.10.10.112:9090:9090"
    volumes:
      - ./prometheus/config/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./prometheus/config/rules:/etc/prometheus/rules:ro
      - ./prometheus/config/secrets:/etc/prometheus/secrets:ro
      - ./prometheus/data:/prometheus
    networks:
      - obs_net
    healthcheck:
      test: ["CMD", "wget", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================================================
  # 3. GRAFANA - Visualization & Dashboards
  # ============================================================================
  # Deploy third - connects to Prometheus
  # Access: http://10.10.10.112:3000 or https://grafana.onurx.com

  grafana:
    image: grafana/grafana:11.4.0  # Latest stable version (2025)
    container_name: grafana
    restart: unless-stopped
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
      - GF_SERVER_ROOT_URL=https://grafana.onurx.com
      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-piechart-panel
      - GF_AUTH_ANONYMOUS_ENABLED=false
      - GF_ANALYTICS_REPORTING_ENABLED=false
      - GF_ANALYTICS_CHECK_FOR_UPDATES=true
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_LOG_MODE=console
      - GF_LOG_LEVEL=info
    ports:
      - "10.10.10.112:3000:3000"
    volumes:
      - ./grafana/data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
      - ./grafana/dashboards:/var/lib/grafana/dashboards:ro
    networks:
      - obs_net
    depends_on:
      - prometheus
      - loki
    healthcheck:
      test: ["CMD", "wget", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================================================
  # 4. LOKI - Log Aggregation
  # ============================================================================
  # Deploy fourth - stores logs locally (can use MinIO later for long-term storage)
  # Access: http://10.10.10.112:3100

  loki:
    image: grafana/loki:3.3.2  # Latest stable version (2025)
    container_name: loki
    restart: unless-stopped
    command: -config.file=/etc/loki/config.yml
    ports:
      - "10.10.10.112:3100:3100"
    volumes:
      - ./loki/config/config.yml:/etc/loki/config.yml:ro
      - ./loki/data:/loki
    networks:
      - obs_net
    healthcheck:
      test: ["CMD", "wget", "--tries=1", "--spider", "http://localhost:3100/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================================================
  # 5. ALLOY - Metrics & Logs Collector
  # ============================================================================
  # Deploy fifth - collects metrics + logs, sends to Prometheus + Loki
  # Web UI: http://10.10.10.112:12345

  alloy:
    image: grafana/alloy:v1.11.2  # Latest stable version (2025)
    container_name: alloy
    restart: unless-stopped
    privileged: true  # Required for system metrics collection
    command:
      - run
      - /etc/alloy/config.alloy
      - --server.http.listen-addr=0.0.0.0:12345
      - --storage.path=/var/lib/alloy/data
    ports:
      - "10.10.10.112:12345:12345"  # Alloy web UI
      # OTLP ports (uncomment when enabling OpenTelemetry receiver in config.alloy)
      # - "10.10.10.112:4317:4317"    # OTLP gRPC (for app instrumentation)
      # - "10.10.10.112:4318:4318"    # OTLP HTTP (for app instrumentation)
    volumes:
      - ./alloy/config/config.alloy:/etc/alloy/config.alloy:ro
      - ./alloy/config/snmp.yml:/etc/alloy/snmp.yml:ro  # SNMP config for Synology
      - /var/run/docker.sock:/var/run/docker.sock:ro  # Docker metrics + logs
      - /:/host/root:ro  # System metrics (CPU, disk, etc.)
      - /sys:/host/sys:ro  # System metrics
      - /proc:/host/proc:ro  # System metrics
      - /var/log:/var/log:ro  # System logs (syslog, auth.log)
      - /var/lib/docker:/var/lib/docker:ro  # Docker container info for cAdvisor
      - /dev/disk:/dev/disk:ro  # Disk info for cAdvisor
      - /sys/fs/cgroup:/sys/fs/cgroup:ro  # cgroups for container metrics
      - ./alloy/data:/var/lib/alloy/data  # Alloy state
    networks:
      - obs_net
    depends_on:
      - prometheus
      - loki
    environment:
      - HOSTNAME=observability
      - HOMEASSISTANT_TOKEN=${HOMEASSISTANT_TOKEN}
    healthcheck:
      test: ["CMD-SHELL", "alloy tools pprof health http://localhost:12345 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================================================
  # PROXMOX EXPORTER - VM Metrics from Hypervisor
  # ============================================================================
  # Exports VM-level metrics from Proxmox VE API
  # Provides hypervisor perspective for all VMs including Synology and HA
  # Access: http://10.10.10.112:9221/metrics

  proxmox-exporter:
    image: prompve/prometheus-pve-exporter:latest
    container_name: proxmox-exporter
    restart: unless-stopped
    ports:
      - "10.10.10.112:9221:9221"
    environment:
      - PVE_HOST=10.10.10.20
      - PVE_USER=monitor@pve
      - PVE_TOKEN_NAME=prometheus
      - PVE_TOKEN_VALUE=59b1017f-7576-454d-bff2-61463f7f0c17
      - PVE_VERIFY_SSL=false
    networks:
      - obs_net
    healthcheck:
      test: ["CMD", "wget", "--tries=1", "--spider", "http://localhost:9221/"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================================================
  # 6. TEMPO - Distributed Tracing Backend
  # ============================================================================
  # Deploy sixth - stores traces in MinIO (S3-compatible storage on db VM)
  # Access: http://10.10.10.112:3200
  # Visualization: Grafana (traces correlated with logs + metrics)

  tempo:
    image: grafana/tempo:2.9.0
    container_name: tempo
    restart: unless-stopped
    command:
      - -config.file=/etc/tempo/tempo.yml
      - -config.expand-env=true
    ports:
      - "10.10.10.112:3200:3200"   # Tempo HTTP
      - "10.10.10.112:4317:4317"   # OTLP gRPC receiver
      - "10.10.10.112:4318:4318"   # OTLP HTTP receiver
    volumes:
      - ./tempo/config/tempo.yml:/etc/tempo/tempo.yml:ro
      - ./tempo/data:/var/tempo
    environment:
      - MINIO_ENDPOINT=${MINIO_ENDPOINT}
      - MINIO_ACCESS_KEY=${MINIO_ACCESS_KEY}
      - MINIO_SECRET_KEY=${MINIO_SECRET_KEY}
    networks:
      - obs_net
    healthcheck:
      test: ["CMD", "wget", "--tries=1", "--spider", "http://localhost:3200/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================================================
  # 7. NETDATA - Real-Time System & Container Monitoring
  # ============================================================================
  # Real-time performance monitoring with 1-second granularity
  # Web UI: http://10.10.10.112:19999
  # Documentation: https://learn.netdata.cloud

  netdata:
    image: netdata/netdata:latest
    container_name: netdata
    hostname: observability
    restart: unless-stopped
    pid: host
    cap_add:
      - SYS_PTRACE
      - SYS_ADMIN
    security_opt:
      - apparmor:unconfined
    ports:
      - "10.10.10.112:19999:19999"
    volumes:
      - ./netdata/config:/etc/netdata
      - ./netdata/lib:/var/lib/netdata
      - ./netdata/cache:/var/cache/netdata
      - /:/host/root:ro,rslave
      - /etc/passwd:/host/etc/passwd:ro
      - /etc/group:/host/etc/group:ro
      - /etc/localtime:/etc/localtime:ro
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /etc/os-release:/host/etc/os-release:ro
      - /var/log:/host/var/log:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - obs_net
    environment:
      - NETDATA_CLAIM_TOKEN=${NETDATA_CLAIM_TOKEN}
      - NETDATA_CLAIM_ROOMS=${NETDATA_CLAIM_ROOMS}
      - NETDATA_CLAIM_URL=https://app.netdata.cloud
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================================================
  # 8. GLANCE - Homelab Dashboard
  # ============================================================================
  # Primary dashboard with RSS, calendar, and widget support for all homelab services
  # Web UI: http://10.10.10.112:8080 or https://home.onurx.com
  # Documentation: https://github.com/glanceapp/glance

  glance:
    image: glanceapp/glance:v0.8.4
    container_name: glance
    restart: unless-stopped
    ports:
      - "10.10.10.112:8080:8080"
    volumes:
      - ./glance/config:/app/config
      - /var/run/docker.sock:/var/run/docker.sock:ro  # Docker widget
    environment:
      - PROXMOX_HOST=${PROXMOX_HOST}
      - PROXMOX_TOKEN_ID=${PROXMOX_TOKEN_ID}
      - PROXMOX_TOKEN_SECRET=${PROXMOX_TOKEN_SECRET}
      - ADGUARD_URL=${ADGUARD_URL}
      - ADGUARD_USERNAME=${ADGUARD_USERNAME}
      - ADGUARD_PASSWORD=${ADGUARD_PASSWORD}
      - SABNZBD_URL=${SABNZBD_URL}
      - SABNZBD_API_KEY=${SABNZBD_API_KEY}
      - SONARR_URL=${SONARR_URL}
      - SONARR_API_KEY=${SONARR_API_KEY}
      - RADARR_URL=${RADARR_URL}
      - RADARR_API_KEY=${RADARR_API_KEY}
      - OVERSEERR_URL=${OVERSEERR_URL}
      - OVERSEERR_API_KEY=${OVERSEERR_API_KEY}
      - TAUTULLI_URL=${TAUTULLI_URL}
      - TAUTULLI_API_KEY=${TAUTULLI_API_KEY}
      - PLEX_URL=${PLEX_URL}
      - PLEX_TOKEN=${PLEX_TOKEN}
      - QBITTORRENT_URL=${QBITTORRENT_URL}
      - PROWLARR_URL=${PROWLARR_URL}
      - PROWLARR_API_KEY=${PROWLARR_API_KEY}
      - BAZARR_URL=${BAZARR_URL}
      - BAZARR_API_KEY=${BAZARR_API_KEY}
    networks:
      - obs_net
    healthcheck:
      test: ["CMD", "wget", "--tries=1", "--spider", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================================================
  # LANGFUSE - AI Observability Platform
  # ============================================================================
  # Track AI calls, costs, latency, and prompt performance
  # Uses external PostgreSQL on db VM (10.10.10.111)
  # Access: https://langfuse.onurx.com
  # Docs: https://langfuse.com/

  langfuse:
    image: langfuse/langfuse:latest
    container_name: langfuse
    restart: unless-stopped
    ports:
      - "10.10.10.112:3001:3000"
    environment:
      # Database (external PostgreSQL)
      - DATABASE_URL=postgresql://${LANGFUSE_DB_USER}:${LANGFUSE_DB_PASSWORD}@10.10.10.111:5432/langfuse

      # ClickHouse (for v3 - external ClickHouse on db VM)
      - CLICKHOUSE_URL=http://${CLICKHOUSE_USER}:${CLICKHOUSE_PASSWORD}@10.10.10.111:8123/langfuse
      - CLICKHOUSE_MIGRATION_URL=clickhouse://${CLICKHOUSE_USER}:${CLICKHOUSE_PASSWORD}@10.10.10.111:9004/langfuse
      - CLICKHOUSE_USER=${CLICKHOUSE_USER}
      - CLICKHOUSE_PASSWORD=${CLICKHOUSE_PASSWORD}

      # NextAuth configuration
      - NEXTAUTH_URL=https://langfuse.onurx.com
      - NEXTAUTH_SECRET=${LANGFUSE_NEXTAUTH_SECRET}

      # Security
      - SALT=${LANGFUSE_SALT}

      # Telemetry (opt-out if you prefer)
      - TELEMETRY_ENABLED=true

      # S3-compatible storage (MinIO) for event recovery
      - S3_ENDPOINT=http://10.10.10.111:9000
      - S3_ACCESS_KEY_ID=${MINIO_ACCESS_KEY}
      - S3_SECRET_ACCESS_KEY=${MINIO_SECRET_KEY}
      - S3_BUCKET_NAME=langfuse-events
      - S3_REGION=us-east-1

    networks:
      - obs_net
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3000/api/public/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

networks:
  obs_net:
    driver: bridge

volumes:
  portainer_data:
  # prometheus_data:
  # grafana_data:
  # loki_data:
