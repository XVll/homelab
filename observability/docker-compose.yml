
# OBSERVABILITY HOST: Komodo + Monitoring Stack
# Build services one at a time in this order:
# 1. Komodo (container management - requires MongoDB from data host)
# 2. Prometheus (metrics storage)
# 3. Grafana (visualization)
# 4. Loki (log aggregation - requires MinIO from data host)
# 5. Alloy (metrics/logs collector)

services:
  # ============================================================================
  # 1. PORTAINER - Container Management UI
  # ============================================================================
  # Deploy FIRST - no dependencies
  # Access: https://10.10.10.112:9443

  portainer:
    image: portainer/portainer-ce:latest
    container_name: portainer
    restart: unless-stopped
    ports:
      - "10.10.10.112:9443:9443"  # HTTPS Web UI
      - "10.10.10.112:8000:8000"  # Edge agent tunnel
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./portainer/data:/data
    networks:
      - obs_net
    # No health check - minimal distroless container without shell/utilities
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================================================
  # 2. PROMETHEUS - Metrics Storage
  # ============================================================================
  # Deploy second - no dependencies
  # Access: http://10.10.10.112:9090 or https://prometheus.onurx.com

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    restart: unless-stopped
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=90d'  # 90 days retention
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-remote-write-receiver'  # Enable remote write (for Alloy)
      - '--web.enable-lifecycle'  # Enable reload via API
    ports:
      - "10.10.10.112:9090:9090"
    volumes:
      - ./prometheus/config/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./prometheus/config/rules:/etc/prometheus/rules:ro
      - ./prometheus/config/secrets:/etc/prometheus/secrets:ro
      - ./prometheus/data:/prometheus
    networks:
      - obs_net
    healthcheck:
      test: ["CMD", "wget", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================================================
  # 3. GRAFANA - Visualization & Dashboards
  # ============================================================================
  # Deploy third - connects to Prometheus
  # Access: http://10.10.10.112:3000 or https://grafana.onurx.com

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    restart: unless-stopped
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
      - GF_SERVER_ROOT_URL=https://grafana.onurx.com
      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-piechart-panel,https://storage.googleapis.com/integration-artifacts/grafana-lokiexplore-app/release/main/any/grafana-lokiexplore-app-main.zip;grafana-lokiexplore-app
      - GF_AUTH_ANONYMOUS_ENABLED=false
      - GF_ANALYTICS_REPORTING_ENABLED=false
      - GF_ANALYTICS_CHECK_FOR_UPDATES=true
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_LOG_MODE=console
      - GF_LOG_LEVEL=info
    ports:
      - "10.10.10.112:3000:3000"
    volumes:
      - ./grafana/data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
      - ./grafana/dashboards:/var/lib/grafana/dashboards:ro
    networks:
      - obs_net
    depends_on:
      - prometheus
      - loki
    healthcheck:
      test: ["CMD", "wget", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================================================
  # 4. LOKI - Log Aggregation
  # ============================================================================
  # Deploy fourth - stores logs locally (can use MinIO later for long-term storage)
  # Access: http://10.10.10.112:3100

  loki:
    image: grafana/loki:latest
    container_name: loki
    restart: unless-stopped
    command: -config.file=/etc/loki/config.yml
    ports:
      - "10.10.10.112:3100:3100"
    volumes:
      - ./loki/config/config.yml:/etc/loki/config.yml:ro
      - ./loki/data:/loki
    networks:
      - obs_net
    healthcheck:
      test: ["CMD", "wget", "--tries=1", "--spider", "http://localhost:3100/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================================================
  # 5. ALLOY - Metrics & Logs Collector
  # ============================================================================
  # Deploy fifth - collects metrics + logs, sends to Prometheus + Loki
  # Web UI: http://10.10.10.112:12345

  alloy:
    image: grafana/alloy:latest
    container_name: alloy
    restart: unless-stopped
    privileged: true  # Required for system metrics collection
    command:
      - run
      - /etc/alloy/config.alloy
      - --server.http.listen-addr=0.0.0.0:12345
      - --storage.path=/var/lib/alloy/data
    ports:
      - "10.10.10.112:12345:12345"  # Alloy web UI
      # OTLP ports (uncomment when enabling OpenTelemetry receiver in config.alloy)
      # - "10.10.10.112:4317:4317"    # OTLP gRPC (for app instrumentation)
      # - "10.10.10.112:4318:4318"    # OTLP HTTP (for app instrumentation)
    volumes:
      - ./alloy/config/config.alloy:/etc/alloy/config.alloy:ro
      - ./alloy/config/snmp.yml:/etc/alloy/snmp.yml:ro  # SNMP config for Synology
      - /var/run/docker.sock:/var/run/docker.sock:ro  # Docker metrics + logs
      - /:/host/root:ro  # System metrics (CPU, disk, etc.)
      - /sys:/host/sys:ro  # System metrics
      - /proc:/host/proc:ro  # System metrics
      - /var/log:/var/log:ro  # System logs (syslog, auth.log)
      - /var/lib/docker:/var/lib/docker:ro  # Docker container info for cAdvisor
      - /dev/disk:/dev/disk:ro  # Disk info for cAdvisor
      - /sys/fs/cgroup:/sys/fs/cgroup:ro  # cgroups for container metrics
      - ./alloy/data:/var/lib/alloy/data  # Alloy state
    networks:
      - obs_net
    depends_on:
      - prometheus
      - loki
    environment:
      - HOSTNAME=observability
      - HOMEASSISTANT_TOKEN=${HOMEASSISTANT_TOKEN}
    healthcheck:
      test: ["CMD-SHELL", "alloy tools pprof health http://localhost:12345 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================================================
  # PROXMOX EXPORTER - VM Metrics from Hypervisor
  # ============================================================================
  # Exports VM-level metrics from Proxmox VE API
  # Provides hypervisor perspective for all VMs including Synology and HA
  # Access: http://10.10.10.112:9221/metrics

  proxmox-exporter:
    image: prompve/prometheus-pve-exporter:latest
    container_name: proxmox-exporter
    restart: unless-stopped
    ports:
      - "10.10.10.112:9221:9221"
    environment:
      - PVE_HOST=10.10.10.20
      - PVE_USER=monitor@pve
      - PVE_TOKEN_NAME=prometheus
      - PVE_TOKEN_VALUE=59b1017f-7576-454d-bff2-61463f7f0c17
      - PVE_VERIFY_SSL=false
    networks:
      - obs_net
    healthcheck:
      test: ["CMD", "wget", "--tries=1", "--spider", "http://localhost:9221/"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================================================
  # 6. TEMPO - Distributed Tracing Backend
  # ============================================================================
  # Deploy sixth - stores traces in MinIO (S3-compatible storage on db VM)
  # Access: http://10.10.10.112:3200
  # Visualization: Grafana (traces correlated with logs + metrics)

  tempo:
    image: grafana/tempo:latest
    container_name: tempo
    restart: unless-stopped
    command:
      - -config.file=/etc/tempo/tempo.yml
      - -config.expand-env=true
    ports:
      - "10.10.10.112:3200:3200"   # Tempo HTTP
      - "10.10.10.112:4317:4317"   # OTLP gRPC receiver
      - "10.10.10.112:4318:4318"   # OTLP HTTP receiver
    volumes:
      - ./tempo/config/tempo.yml:/etc/tempo/tempo.yml:ro
      - ./tempo/data:/var/tempo
    environment:
      - MINIO_ENDPOINT=${MINIO_ENDPOINT}
      - MINIO_ACCESS_KEY=${MINIO_ACCESS_KEY}
      - MINIO_SECRET_KEY=${MINIO_SECRET_KEY}
    networks:
      - obs_net
    healthcheck:
      test: ["CMD", "wget", "--tries=1", "--spider", "http://localhost:3200/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================================================
  # 7. NETDATA - Real-Time System & Container Monitoring
  # ============================================================================
  # Real-time performance monitoring with 1-second granularity
  # Web UI: http://10.10.10.112:19999
  # Documentation: https://learn.netdata.cloud

  netdata:
    image: netdata/netdata:latest
    container_name: netdata
    hostname: observability
    restart: unless-stopped
    pid: host
    cap_add:
      - SYS_PTRACE
      - SYS_ADMIN
    security_opt:
      - apparmor:unconfined
    ports:
      - "10.10.10.112:19999:19999"
    volumes:
      - ./netdata/config:/etc/netdata
      - ./netdata/lib:/var/lib/netdata
      - ./netdata/cache:/var/cache/netdata
      - /:/host/root:ro,rslave
      - /etc/passwd:/host/etc/passwd:ro
      - /etc/group:/host/etc/group:ro
      - /etc/localtime:/etc/localtime:ro
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /etc/os-release:/host/etc/os-release:ro
      - /var/log:/host/var/log:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - obs_net
    environment:
      - NETDATA_CLAIM_TOKEN=${NETDATA_CLAIM_TOKEN}
      - NETDATA_CLAIM_ROOMS=${NETDATA_CLAIM_ROOMS}
      - NETDATA_CLAIM_URL=https://app.netdata.cloud
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================================================
  # 8. GLANCE - Homelab Dashboard
  # ============================================================================
  # Primary dashboard with RSS, calendar, and widget support for all homelab services
  # Web UI: http://10.10.10.112:8080 or https://home.onurx.com
  # Documentation: https://github.com/glanceapp/glance

  glance:
    image: glanceapp/glance:v0.8.4
    container_name: glance
    restart: unless-stopped
    ports:
      - "10.10.10.112:8080:8080"
    volumes:
      - ./glance/config:/app/config
      - /var/run/docker.sock:/var/run/docker.sock:ro  # Docker widget
    environment:
      - PROXMOX_HOST=${PROXMOX_HOST}
      - PROXMOX_TOKEN_ID=${PROXMOX_TOKEN_ID}
      - PROXMOX_TOKEN_SECRET=${PROXMOX_TOKEN_SECRET}
      - ADGUARD_URL=${ADGUARD_URL}
      - ADGUARD_USERNAME=${ADGUARD_USERNAME}
      - ADGUARD_PASSWORD=${ADGUARD_PASSWORD}
      - SABNZBD_URL=${SABNZBD_URL}
      - SABNZBD_API_KEY=${SABNZBD_API_KEY}
      - SONARR_URL=${SONARR_URL}
      - SONARR_API_KEY=${SONARR_API_KEY}
      - RADARR_URL=${RADARR_URL}
      - RADARR_API_KEY=${RADARR_API_KEY}
      - OVERSEERR_URL=${OVERSEERR_URL}
      - OVERSEERR_API_KEY=${OVERSEERR_API_KEY}
      - TAUTULLI_URL=${TAUTULLI_URL}
      - TAUTULLI_API_KEY=${TAUTULLI_API_KEY}
      - PLEX_URL=${PLEX_URL}
      - PLEX_TOKEN=${PLEX_TOKEN}
      - QBITTORRENT_URL=${QBITTORRENT_URL}
      - PROWLARR_URL=${PROWLARR_URL}
      - PROWLARR_API_KEY=${PROWLARR_API_KEY}
      - BAZARR_URL=${BAZARR_URL}
      - BAZARR_API_KEY=${BAZARR_API_KEY}
    networks:
      - obs_net
    healthcheck:
      test: ["CMD", "wget", "--tries=1", "--spider", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================================================
  # LANGFUSE - AI Observability Platform
  # ============================================================================
  # Track AI calls, costs, latency, and prompt performance
  # Uses external PostgreSQL on db VM (10.10.10.111)
  # Access: https://langfuse.onurx.com
  # Docs: https://langfuse.com/

  langfuse:
    image: langfuse/langfuse:latest
    container_name: langfuse
    restart: unless-stopped
    ports:
      - "10.10.10.112:3001:3000"
    environment:
      # Database (external PostgreSQL)
      - DATABASE_URL=postgresql://${LANGFUSE_DB_USER}:${LANGFUSE_DB_PASSWORD}@10.10.10.111:5432/langfuse

      # ClickHouse (for v3 - external ClickHouse on db VM)
      - CLICKHOUSE_URL=http://${CLICKHOUSE_USER}:${CLICKHOUSE_PASSWORD}@10.10.10.111:8123/langfuse
      - CLICKHOUSE_MIGRATION_URL=clickhouse://${CLICKHOUSE_USER}:${CLICKHOUSE_PASSWORD}@10.10.10.111:9004
      - CLICKHOUSE_USER=${CLICKHOUSE_USER}
      - CLICKHOUSE_PASSWORD=${CLICKHOUSE_PASSWORD}
      - CLICKHOUSE_DB=langfuse
      - CLICKHOUSE_CLUSTER_ENABLED=false

      # NextAuth configuration
      - NEXTAUTH_URL=https://langfuse.onurx.com
      - NEXTAUTH_SECRET=${LANGFUSE_NEXTAUTH_SECRET}

      # Security
      - SALT=${LANGFUSE_SALT}
      - ENCRYPTION_KEY=${LANGFUSE_ENCRYPTION_KEY}

      # Telemetry (opt-out if you prefer)
      - TELEMETRY_ENABLED=true

      # Redis - Cache & Queue (from db VM)
      - REDIS_HOST=10.10.10.111
      - REDIS_PORT=6379
      - REDIS_AUTH=${REDIS_PASSWORD}

      # S3-compatible storage (MinIO) for event recovery
      - LANGFUSE_S3_EVENT_UPLOAD_ENABLED=true
      - LANGFUSE_S3_EVENT_UPLOAD_BUCKET=langfuse-events
      - LANGFUSE_S3_EVENT_UPLOAD_REGION=us-east-1
      - LANGFUSE_S3_EVENT_UPLOAD_ENDPOINT=http://10.10.10.111:9000
      - LANGFUSE_S3_EVENT_UPLOAD_ACCESS_KEY_ID=${MINIO_ACCESS_KEY}
      - LANGFUSE_S3_EVENT_UPLOAD_SECRET_ACCESS_KEY=${MINIO_SECRET_KEY}
      - LANGFUSE_S3_EVENT_UPLOAD_FORCE_PATH_STYLE=true

    networks:
      - obs_net
    # No health check - langfuse image doesn't include wget/curl
    # Service health monitored via port availability
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================================================
  # HYPERDX - Full-Stack Observability Platform
  # ============================================================================
  # Logs, traces, metrics, and session replay in one platform
  # Uses external MongoDB and ClickHouse from db VM (10.10.10.111)
  # Access: https://hyperdx.onurx.com
  # OTLP endpoint: http://10.10.10.112:4318 (HTTP) or :4317 (gRPC)
  # Docs: https://github.com/hyperdxio/hyperdx

  hyperdx-otel-collector:
    image: docker.hyperdx.io/hyperdx/hyperdx-otel-collector:2
    container_name: hyperdx-otel-collector
    restart: unless-stopped
    ports:
      - "10.10.10.112:13133:13133"  # health_check extension
      - "10.10.10.112:24225:24225"  # fluentd receiver
      # Note: 4317/4318 already used by Tempo, HyperDX collector will use different ports
      - "10.10.10.112:14317:4317"   # OTLP gRPC receiver (remapped to avoid Tempo conflict)
      - "10.10.10.112:14318:4318"   # OTLP HTTP receiver (remapped to avoid Tempo conflict)
      - "10.10.10.112:8889:8888"    # metrics extension (remapped to avoid conflicts)
    environment:
      - CLICKHOUSE_ENDPOINT=tcp://10.10.10.111:9000?dial_timeout=30s
      - CLICKHOUSE_USER=${CLICKHOUSE_USER}
      - CLICKHOUSE_PASSWORD=${CLICKHOUSE_PASSWORD}
      - HYPERDX_OTEL_EXPORTER_CLICKHOUSE_DATABASE=${HYPERDX_CLICKHOUSE_DB}
      - HYPERDX_LOG_LEVEL=${HYPERDX_LOG_LEVEL}
      - OPAMP_SERVER_URL=http://hyperdx-app:${HYPERDX_OPAMP_PORT}
    networks:
      - obs_net
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  hyperdx-app:
    image: docker.hyperdx.io/hyperdx/hyperdx:2
    container_name: hyperdx-app
    restart: unless-stopped
    ports:
      - "10.10.10.112:8002:8000"    # API port (remapped to avoid Portainer conflict)
      - "10.10.10.112:8081:8080"    # Web UI port (remapped from 8080 to avoid Glance conflict)
    environment:
      # App URLs
      - FRONTEND_URL=${HYPERDX_APP_URL}
      - HYPERDX_API_PORT=8000
      - HYPERDX_APP_PORT=8080
      - HYPERDX_APP_URL=${HYPERDX_APP_URL}
      - HYPERDX_LOG_LEVEL=${HYPERDX_LOG_LEVEL}
      - HYPERDX_OPAMP_PORT=${HYPERDX_OPAMP_PORT}
      - HYPERDX_BASE_PATH=

      # External MongoDB (db VM) - using hyperdx user
      - MONGO_URI=mongodb://${HYPERDX_MONGO_USER}:${HYPERDX_MONGO_PASSWORD}@10.10.10.111:27017/hyperdx

      # Server URL
      - SERVER_URL=http://127.0.0.1:8000

      # OTLP Configuration
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://hyperdx-otel-collector:4318
      - OTEL_SERVICE_NAME=hyperdx-app

      # Usage stats
      - USAGE_STATS_ENABLED=false

      # ClickHouse connection (external on db VM)
      - DEFAULT_CONNECTIONS=[{"name":"Homelab ClickHouse","host":"http://10.10.10.111:8123","username":"${CLICKHOUSE_USER}","password":"${CLICKHOUSE_PASSWORD}"}]

      # Default data sources
      - DEFAULT_SOURCES=[{"from":{"databaseName":"${HYPERDX_CLICKHOUSE_DB}","tableName":"otel_logs"},"kind":"log","timestampValueExpression":"TimestampTime","name":"Logs","displayedTimestampValueExpression":"Timestamp","implicitColumnExpression":"Body","serviceNameExpression":"ServiceName","bodyExpression":"Body","eventAttributesExpression":"LogAttributes","resourceAttributesExpression":"ResourceAttributes","defaultTableSelectExpression":"Timestamp,ServiceName,SeverityText,Body","severityTextExpression":"SeverityText","traceIdExpression":"TraceId","spanIdExpression":"SpanId","connection":"Homelab ClickHouse","traceSourceId":"Traces","sessionSourceId":"Sessions","metricSourceId":"Metrics"},{"from":{"databaseName":"${HYPERDX_CLICKHOUSE_DB}","tableName":"otel_traces"},"kind":"trace","timestampValueExpression":"Timestamp","name":"Traces","displayedTimestampValueExpression":"Timestamp","implicitColumnExpression":"SpanName","serviceNameExpression":"ServiceName","bodyExpression":"SpanName","eventAttributesExpression":"SpanAttributes","resourceAttributesExpression":"ResourceAttributes","defaultTableSelectExpression":"Timestamp,ServiceName,StatusCode,round(Duration/1e6),SpanName","traceIdExpression":"TraceId","spanIdExpression":"SpanId","durationExpression":"Duration","durationPrecision":9,"parentSpanIdExpression":"ParentSpanId","spanNameExpression":"SpanName","spanKindExpression":"SpanKind","statusCodeExpression":"StatusCode","statusMessageExpression":"StatusMessage","connection":"Homelab ClickHouse","logSourceId":"Logs","sessionSourceId":"Sessions","metricSourceId":"Metrics"},{"from":{"databaseName":"${HYPERDX_CLICKHOUSE_DB}","tableName":""},"kind":"metric","timestampValueExpression":"TimeUnix","name":"Metrics","resourceAttributesExpression":"ResourceAttributes","metricTables":{"gauge":"otel_metrics_gauge","histogram":"otel_metrics_histogram","sum":"otel_metrics_sum"},"connection":"Homelab ClickHouse","logSourceId":"Logs","traceSourceId":"Traces","sessionSourceId":"Sessions"},{"from":{"databaseName":"${HYPERDX_CLICKHOUSE_DB}","tableName":"hyperdx_sessions"},"kind":"session","timestampValueExpression":"TimestampTime","name":"Sessions","displayedTimestampValueExpression":"Timestamp","implicitColumnExpression":"Body","serviceNameExpression":"ServiceName","bodyExpression":"Body","eventAttributesExpression":"LogAttributes","resourceAttributesExpression":"ResourceAttributes","defaultTableSelectExpression":"Timestamp,ServiceName,SeverityText,Body","severityTextExpression":"SeverityText","traceIdExpression":"TraceId","spanIdExpression":"SpanId","connection":"Homelab ClickHouse","logSourceId":"Logs","traceSourceId":"Traces","metricSourceId":"Metrics"}]
    networks:
      - obs_net
    depends_on:
      - hyperdx-otel-collector
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

networks:
  obs_net:
    driver: bridge

volumes:
  portainer_data:
  # prometheus_data:
  # grafana_data:
  # loki_data:
