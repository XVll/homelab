# ============================================================================
# LITELLM CONFIGURATION
# ============================================================================
# Unified gateway for multiple LLM providers
# Docs: https://docs.litellm.ai/docs/proxy/configs
# ============================================================================

model_list:
  # Models are managed via database UI (STORE_MODEL_IN_DB=True)
  # Add models at https://litellm.onurx.com

# Router settings
router_settings:
  routing_strategy: usage-based-routing  # or latency-based-routing, simple-shuffle
  num_retries: 3
  timeout: 600  # 10 minutes for long-running requests
  fallbacks: []  # Add fallback models if needed

# General settings
general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  database_url: os.environ/DATABASE_URL

  # Cost tracking
  track_cost_per_deployment: true

  # Caching (requires Redis)
  cache: true
  cache_type: redis

  # Rate limiting
  max_parallel_requests: 1000
  global_max_parallel_requests: 1000

# Logging
litellm_settings:
  # Log LLM API calls to Langfuse
  success_callback: ["langfuse"]
  failure_callback: ["langfuse"]

  # Callbacks configuration for Langfuse
  langfuse_public_key: os.environ/LANGFUSE_PUBLIC_KEY
  langfuse_secret_key: os.environ/LANGFUSE_SECRET_KEY
  langfuse_host: os.environ/LANGFUSE_HOST
